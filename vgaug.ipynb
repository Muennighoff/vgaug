{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.\n\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\nimport albumentations\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\nimport torch\nfrom torch.utils.data import Dataset\n\nfrom tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Testing"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"## 1st place ISIC competition\n\ndef get_transforms(image_size):\n\n    transforms_train = albumentations.Compose([\n        albumentations.Transpose(p=0.5),\n        albumentations.VerticalFlip(p=0.5),\n        albumentations.HorizontalFlip(p=0.5),\n        albumentations.RandomBrightness(limit=0.2, p=0.75),\n        albumentations.RandomContrast(limit=0.2, p=0.75),\n        albumentations.OneOf([\n            albumentations.MotionBlur(blur_limit=5),\n            albumentations.MedianBlur(blur_limit=5),\n            albumentations.GaussianBlur(blur_limit=5),\n            albumentations.GaussNoise(var_limit=(5.0, 30.0)),\n        ], p=0.7),\n\n        albumentations.OneOf([\n            albumentations.OpticalDistortion(distort_limit=1.0),\n            albumentations.GridDistortion(num_steps=5, distort_limit=1.),\n            albumentations.ElasticTransform(alpha=3),\n        ], p=0.7),\n\n        albumentations.CLAHE(clip_limit=4.0, p=0.7),\n        albumentations.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=10, p=0.5),\n        albumentations.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, border_mode=0, p=0.85),\n        albumentations.Resize(image_size, image_size),\n        albumentations.Cutout(max_h_size=int(image_size * 0.375), max_w_size=int(image_size * 0.375), num_holes=1, p=0.7),\n        albumentations.Normalize()\n    ])\n    \n    \n    transforms_val = albumentations.Compose([\n        albumentations.Resize(image_size, image_size),\n        albumentations.Normalize()\n    ])\n\n    return transforms_train, transforms_val","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"BEFORE:\")\nim_gray = cv2.cvtColor(image_or, cv2.COLOR_BGR2GRAY)\nprint(im_gray.shape)\nplt.imshow(im_gray, cmap='gray')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = \"../input/num110/num/0.png\"\n\nimage_or = cv2.imread(path)\ntransforms_train, transforms_val = get_transforms(28)\n\nres = transforms_train(image=image_or)\nimage = res['image']\nprint(image.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"AFTER:\")\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2GRAY),cmap=\"gray\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# STOPSIGNS"},{"metadata":{"trusted":true},"cell_type":"code","source":"## 1st place ISIC competition\n\ndef get_transforms(image_size):\n\n    transforms_train = albumentations.Compose([\n        albumentations.Transpose(p=0.5),\n        albumentations.VerticalFlip(p=0.5),\n        albumentations.HorizontalFlip(p=0.5),\n        albumentations.RandomBrightness(limit=0.1, p=0.75),\n        albumentations.RandomContrast(limit=0.1, p=0.75),\n        albumentations.OneOf([\n            albumentations.MotionBlur(blur_limit=3),\n            albumentations.MedianBlur(blur_limit=3),\n            albumentations.GaussianBlur(blur_limit=2),\n            albumentations.GaussNoise(var_limit=(5.0, 15.0)),\n        ], p=0.7),\n\n        #albumentations.OneOf([\n        #    albumentations.OpticalDistortion(distort_limit=0.2),\n        #    albumentations.GridDistortion(num_steps=1, distort_limit=1.),\n        #    albumentations.ElasticTransform(alpha=1),\n        #], p=0.7),\n\n        #albumentations.CLAHE(clip_limit=1.0, p=0.3),\n        #albumentations.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=10, p=0.5),\n        albumentations.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, border_mode=0, p=0.85),\n        albumentations.Resize(image_size, image_size),\n        albumentations.Cutout(max_h_size=int(image_size * 0.1), max_w_size=int(image_size * 0.1), num_holes=1, p=0.7),\n        albumentations.Normalize()\n    ])\n    \n    transforms_val = albumentations.Compose([\n        albumentations.Resize(image_size, image_size),\n        albumentations.Normalize()\n    ])\n\n    return transforms_train, transforms_val","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = \"../input/stopsign/stop.png\"\n\nimage_or = cv2.imread(path)\ntransforms_train, transforms_val = get_transforms(30)\n\nres = transforms_train(image=image_or)\nimage = res['image']\nprint(image.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"BEFORE:\")\nim_gray = cv2.cvtColor(image_or, cv2.COLOR_BGR2RGB)\nprint(im_gray.shape)\nplt.imshow(im_gray)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"AFTER:\")\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir ./stop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = \"../input/stopsign/stop.png\"\n\n\nfor i in range(1000):\n    \n    image_or = cv2.imread(path)\n    transforms_train, transforms_val = get_transforms(30)\n\n    res = transforms_train(image=image_or)\n    image = res['image']\n\n    image_x = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image_x = (image_x - np.min(image_x))/np.ptp(image_x)\n    plt.imsave(\"./stop/\" + str(i) + \".png\", image_x)\n    \n    if i % 50 == 0:\n        print(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!zip -r ./stop.zip ./stop","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"  \nimport cv2\nimport numpy as np\nimport os\nimport sys\nimport tensorflow as tf\nimport timeit\n\nfrom sklearn.model_selection import train_test_split\n\nEPOCHS = 5\nIMG_WIDTH = 30\nIMG_HEIGHT = 30\nNUM_CATEGORIES = 43 ##############\nTEST_SIZE = 0.4\n\n\ndef main(data_path=None, aug_path=\"../input/vecaugsigns/30w\", filename=\"model.pth\", augment=True):\n    \n    \n    np.random.seed(2)\n    tf.random.set_seed(2)\n\n    # Check command-line arguments\n    if len(sys.argv) not in [2, 3]:\n        sys.exit(\"Usage: python traffic.py data_directory [model.h5]\")\n    \n    # Get image arrays and labels for all image files\n    images, labels = load_data(data_path, augment=augment)\n    \n    # t = timeit.Timer(\"images, labels = load_data(sys.argv[1])\", \"from __main__ import load_data\")\n    # print(t.timeit(number=1))\n    \n    \n    # Split data into training and testing sets\n    labels = tf.keras.utils.to_categorical(labels)\n    x_train, x_test, y_train, y_test = train_test_split(\n        np.array(images), np.array(labels), test_size=TEST_SIZE\n    )\n    \n    \n    # Add augmented to train\n    if augment:\n        images, labels = load_vgexp(aug_path)\n        # Turn labels into array of shape x, num_categories\n        labels = np.zeros((len(labels), NUM_CATEGORIES))\n        labels[:, 14] = 1\n        x_train = np.concatenate((x_train, np.array(images)), axis=0)\n        y_train = np.concatenate((y_train, labels), axis=0)\n\n    # Add original stops to test\n    images, labels = load_data(data_path, skip=True, augment=augment)\n    # Turn labels into array of shape x, num_categories\n    labels = np.zeros((len(labels), NUM_CATEGORIES))\n    labels[:, 14] = 1 \n    x_test = np.concatenate((x_train, np.array(images)), axis=0)\n    y_test = np.concatenate((y_train, labels), axis=0)\n    # Alternative, testing only on stop:\n    x_test = np.array(images)\n    y_test = labels\n    \n\n    # Get a compiled neural network\n    model = get_model()\n\n    # Fit model on training data\n    model.fit(x_train, y_train, epochs=EPOCHS)\n\n    # Evaluate neural network performance\n    model.evaluate(x_test, y_test, verbose=2)\n\n    model.summary()\n\n    # Save model to file\n    model.save(filename)\n    print(f\"Model saved to {filename}.\")\n\n\ndef load_vgexp(aug_path=\"../input/vecaugsigns/30w\"):\n    \"\"\"\n    Loads augmented stop signs to training.\n    \n    aug_path: path to 1K augmented stop images\n    \"\"\"\n    \n    images = []\n    labels = []\n    \n    # Do our images work?\n    for ppm_file in os.listdir(aug_path):\n        raw_ppm = cv2.imread(os.path.join(aug_path, ppm_file))\n        resized_ppm = cv2.resize(raw_ppm, dsize=(IMG_WIDTH, IMG_HEIGHT), interpolation=cv2.INTER_AREA)\n        # Append image & label\n        images.append(resized_ppm)\n        labels.append(14)\n                \n        \n    return (images, labels)\n    \n    \n\ndef load_data(data_dir, skip=False, augment=False):\n    \"\"\"\n    Load image data from directory `data_dir`.\n    Assume `data_dir` has one directory named after each category, numbered\n    0 through NUM_CATEGORIES - 1. Inside each category directory will be some\n    number of image files.\n    Return tuple `(images, labels)`. `images` should be a list of all\n    of the images in the data directory, where each image is formatted as a\n    numpy ndarray with dimensions IMG_WIDTH x IMG_HEIGHT x 3. `labels` should\n    be a list of integer labels, representing the categories for each of the\n    corresponding `images`.\n    \"\"\"\n    # Declare image & labels list - We later convert the images list into an np array - No we don't!\n    # We return a list of NP Arrays!\n    # Also we will not label each group of images but EACH image \n    images = []\n    labels = []\n\n    # Iterate through each category in directory\n    for category_folder in os.listdir(data_dir):\n        # Skip .DS_File\n        if category_folder.startswith(\".\"):\n            continue\n            \n        if skip:\n            # Skip all which are not 14\n            if not(category_folder.startswith(\"14\")):\n                continue\n        else:\n            # If augmenting skip 14 in original loading\n            if augment==True:\n                if category_folder.startswith(\"14\"):\n                    continue\n            \n\n        # Get the path to each category's folder\n        image_folder = os.path.join(data_dir, category_folder)\n\n        # Iterate through the directory of each label\n        for ppm_file in os.listdir(image_folder):\n            # Extract the ppm file and resize it - No need to focus on the 3 channels here, since imread takes them by default \n            raw_ppm = cv2.imread(os.path.join(image_folder, ppm_file))\n            resized_ppm = cv2.resize(raw_ppm, dsize=(IMG_WIDTH, IMG_HEIGHT), interpolation=cv2.INTER_AREA)\n            # Append image & label\n            images.append(resized_ppm)\n            labels.append(category_folder)\n        \n\n    # Return a tuple w/ list of images & list of labels (->43*amount of images per folder labels)\n    return (images, labels)\n\n\ndef get_model():\n    \"\"\"\n    Returns a compiled convolutional neural network model. Assume that the\n    `input_shape` of the first layer is `(IMG_WIDTH, IMG_HEIGHT, 3)`.\n    The output layer should have `NUM_CATEGORIES` units, one for each category.\n    \"\"\"\n\n    model = tf.keras.models.Sequential([\n        \n        # First Convoluational layer to create 32 feature maps with a kernel size of 3,3 (More fm's gave me worse accuracy)\n        tf.keras.layers.Conv2D(32, (3,3), activation=\"relu\", input_shape=(IMG_WIDTH, IMG_HEIGHT, 3), use_bias=True),\n\n        # Pool by selecting highest values from 2x2 matrix\n        tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n\n        # Flatten into single dimension\n        tf.keras.layers.Flatten(),\n\n        # First Dropout\n        tf.keras.layers.Dropout(0.3),\n\n        # Create dense NN - 1 hidden layer is generally enough\n        tf.keras.layers.Dense(129, activation=\"relu\"),\n\n        # Second Dropout\n        tf.keras.layers.Dropout(0.03),\n\n        # Create Output layer\n        tf.keras.layers.Dense(43, activation=\"softmax\")\n\n        ])\n\n    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n\n\n    return model","execution_count":84,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Base\n#main(\"../input/trafficsigns/gtsrb\", augment=False)\n# Vg augmented\n#main(\"../input/trafficsigns/gtsrb\", aug_path=\"../input/vecaugsigns/30w\", augment=True)\n# Bitmap augmented\nmain(\"../input/trafficsigns/gtsrb\", aug_path=\"../input/vgaug/stop\", augment=True)","execution_count":86,"outputs":[{"output_type":"stream","text":"Epoch 1/5\n531/531 [==============================] - 7s 14ms/step - loss: 6.0507 - accuracy: 0.1506\nEpoch 2/5\n531/531 [==============================] - 7s 14ms/step - loss: 2.7748 - accuracy: 0.2440\nEpoch 3/5\n531/531 [==============================] - 7s 13ms/step - loss: 2.1859 - accuracy: 0.3543\nEpoch 4/5\n531/531 [==============================] - 7s 14ms/step - loss: 1.5999 - accuracy: 0.4982\nEpoch 5/5\n531/531 [==============================] - 8s 15ms/step - loss: 1.2968 - accuracy: 0.5813\n17/17 - 0s - loss: 0.0912 - accuracy: 0.9741\nModel: \"sequential_29\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_29 (Conv2D)           (None, 28, 28, 32)        896       \n_________________________________________________________________\nmax_pooling2d_29 (MaxPooling (None, 14, 14, 32)        0         \n_________________________________________________________________\nflatten_29 (Flatten)         (None, 6272)              0         \n_________________________________________________________________\ndropout_58 (Dropout)         (None, 6272)              0         \n_________________________________________________________________\ndense_58 (Dense)             (None, 129)               809217    \n_________________________________________________________________\ndropout_59 (Dropout)         (None, 129)               0         \n_________________________________________________________________\ndense_59 (Dense)             (None, 43)                5590      \n=================================================================\nTotal params: 815,703\nTrainable params: 815,703\nNon-trainable params: 0\n_________________________________________________________________\nModel saved to model.pth.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make sure both are augmented to 1000 files\n!cd ../input/vgaug/stop; ls -1 | wc -l\n!cd ../input/vecaugsigns/30w/; ls -1 | wc -l","execution_count":72,"outputs":[{"output_type":"stream","text":"1000\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"Results:\n    \n\nSeed 1:\nBASE - Original stop shields in test in training & test\nEpoch 1/5\n500/500 [==============================] - 7s 14ms/step - loss: 2.8991 - accuracy: 0.5000\nEpoch 2/5\n500/500 [==============================] - 7s 13ms/step - loss: 0.6984 - accuracy: 0.8114\nEpoch 3/5\n500/500 [==============================] - 7s 14ms/step - loss: 0.4682 - accuracy: 0.8766\nEpoch 4/5\n500/500 [==============================] - 7s 13ms/step - loss: 0.3476 - accuracy: 0.9107\nEpoch 5/5\n500/500 [==============================] - 7s 13ms/step - loss: 0.3610 - accuracy: 0.9070\nTEST: 17/17 - 0s - loss: 0.0053 - accuracy: 0.9963\n        \nAug_VectorGraphics - AugStop shields in training; Original stop shields in test\nEpoch 1/5\n521/521 [==============================] - 7s 13ms/step - loss: 2.8530 - accuracy: 0.5070\nEpoch 2/5\n521/521 [==============================] - 7s 14ms/step - loss: 0.6967 - accuracy: 0.8168\nEpoch 3/5\n521/521 [==============================] - 7s 14ms/step - loss: 0.4639 - accuracy: 0.8782\nEpoch 4/5\n521/521 [==============================] - 7s 13ms/step - loss: 0.3817 - accuracy: 0.9001\nEpoch 5/5\n521/521 [==============================] - 7s 14ms/step - loss: 0.3239 - accuracy: 0.9158\nTEST: 17/17 - 0s - loss: 5.6308 - accuracy: 0.2333\n        \n        \nAug_Bitmap - Aug Bitmap stop shields in training; Original stop shields in test\n14\nEpoch 1/5\n521/521 [==============================] - 7s 13ms/step - loss: 2.7158 - accuracy: 0.5508\nEpoch 2/5\n521/521 [==============================] - 7s 14ms/step - loss: 0.6372 - accuracy: 0.8331\nEpoch 3/5\n521/521 [==============================] - 7s 14ms/step - loss: 0.4173 - accuracy: 0.8884\nEpoch 4/5\n521/521 [==============================] - 7s 14ms/step - loss: 0.3833 - accuracy: 0.9034\nEpoch 5/5\n521/521 [==============================] - 7s 13ms/step - loss: 0.3194 - accuracy: 0.9211\nTEST: 17/17 - 0s - loss: 0.2484 - accuracy: 0.9500\nModel: \"sequential_24\"\n    \n\nSame order for Seed 2:\n    \nBASE:\nEpoch 1/5\n500/500 [==============================] - 7s 13ms/step - loss: 6.0909 - accuracy: 0.1785\nEpoch 2/5\n500/500 [==============================] - 7s 14ms/step - loss: 1.2191 - accuracy: 0.6929\nEpoch 3/5\n500/500 [==============================] - 7s 13ms/step - loss: 0.6288 - accuracy: 0.8403\nEpoch 4/5\n500/500 [==============================] - 7s 14ms/step - loss: 0.4524 - accuracy: 0.8819\nEpoch 5/5\n500/500 [==============================] - 7s 14ms/step - loss: 0.3456 - accuracy: 0.9090\nTEST: 17/17 - 0s - loss: 0.0098 - accuracy: 0.9963\nModel: \"sequential_27\"\n    \n\nVectorGraphics:\n14\nEpoch 1/5\n521/521 [==============================] - 7s 13ms/step - loss: 6.0746 - accuracy: 0.1644\nEpoch 2/5\n521/521 [==============================] - 7s 13ms/step - loss: 2.4584 - accuracy: 0.3501\nEpoch 3/5\n521/521 [==============================] - 7s 14ms/step - loss: 1.1894 - accuracy: 0.6577\nEpoch 4/5\n521/521 [==============================] - 8s 14ms/step - loss: 0.7921 - accuracy: 0.7714\nEpoch 5/5\n521/521 [==============================] - 7s 14ms/step - loss: 0.6189 - accuracy: 0.8222\nTEST: 17/17 - 0s - loss: 12.2995 - accuracy: 0.0407\n    \nBITMAP:\nEpoch 1/5\n521/521 [==============================] - 7s 14ms/step - loss: 6.1305 - accuracy: 0.1436\nEpoch 2/5\n521/521 [==============================] - 7s 13ms/step - loss: 2.6992 - accuracy: 0.2950\nEpoch 3/5\n521/521 [==============================] - 7s 14ms/step - loss: 1.4741 - accuracy: 0.6032\nEpoch 4/5\n521/521 [==============================] - 7s 14ms/step - loss: 0.8503 - accuracy: 0.7692\nEpoch 5/5\n521/521 [==============================] - 7s 13ms/step - loss: 0.6088 - accuracy: 0.8328\nTEST: 17/17 - 0s - loss: 5.2933 - accuracy: 0.2352\nModel: \"sequential_25\"\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}